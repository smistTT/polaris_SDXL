# SPDX-FileCopyrightText: (C) 2025 Tenstorrent AI ULC
# SPDX-License-Identifier: Apache-2.0
workloads:
  - api: TTSIM
    name: basic_llm
    basedir: workloads
    module : BasicLLM.py
    params :
      vocab_sz             : 50257
      drop_prob            : 0.1
      attn_type            : causal
      norm_type            : layer
      positional_encoding  : learned
      use_segment_embedding: False
      bs                   : 1
    instances:
      gpt_nano  : { nL:  3, nH:  3, dE:   48, nW:   32}
      gpt_micro : { nL:  4, nH:  4, dE:  128, nW:   32}
      gpt_mini  : { nL:  6, nH:  6, dE:  192, nW:   32}
      gpt1      : { nL: 12, nH: 12, dE:  768, nW:  512}
      gpt2_m    : { nL: 24, nH: 16, dE:  768, nW:  768}
      gpt2_l    : { nL: 36, nH: 20, dE: 1280, nW: 1024}
  - api: TTSIM
    name: GPTJ
    basedir: workloads
    module : BasicLLM.py
    params :
      vocab_sz             : 50400
      drop_prob            : 0.1
      attn_type            : causal
      use_bias             : False
      norm_type            : layer
      positional_encoding  : learned
      use_segment_embedding: False
      bs                   : 1
    instances:
      gpt_j     : { nL: 28, nH: 16, dE: 4096, nW: 2048}
  - api: TTSIM
    name: BERT
    basedir: workloads
    module : BasicLLM.py
    params :
      vocab_sz             : 30522
      drop_prob            : 0.1
      attn_type            : bidir
      norm_type            : layer
      positional_encoding  : learned
      use_segment_embedding: True
      bs                   : 1
    instances:
      bert_base : { nL: 12, nH: 12, dE:  768, nW:  512}
      bert_large: { nL: 24, nH: 16, dE: 1024, nW:  512}
  - api: TTSIM
    name: sdxl_unet
    basedir: workloads
    module : SDXLUNet.py
    params :
      # Core Architecture Parameters
      sample_size: 128              # For 1024x1024 output (128 * 8 = 1024)
      in_channels: 4                # Input latent channels
      out_channels: 4               # Output latent channels
      center_input_sample: false
      flip_sin_to_cos: true
      freq_shift: 0
      
      # Block Configuration
      down_block_types: ["CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "DownBlock2D"]
      mid_block_type: "UNetMidBlock2DCrossAttn" 
      up_block_types: ["UpBlock2D", "CrossAttnUpBlock2D", "CrossAttnUpBlock2D", "CrossAttnUpBlock2D"]
      
      # Channel and Layer Configuration
      block_out_channels: [320, 640, 1280, 1280]
      layers_per_block: 2
      downsample_padding: 1
      mid_block_scale_factor: 1
      dropout: 0.0
      
      # Activation and Normalization
      act_fn: "silu"
      norm_num_groups: 32
      norm_eps: 1e-5
      
      # SDXL-Specific Features
      cross_attention_dim: 2048     # Dual text encoder embeddings
      transformer_layers_per_block: [1, 2, 10, 10]
      attention_head_dim: [5, 10, 20, 20]
      use_linear_projection: true
      
      # Additional Embeddings
      addition_embed_type: "text_time"
      addition_time_embed_dim: 256
      projection_class_embeddings_input_dim: 2816  # 5*256 + 1280 + 256
      
      # Memory and Compute
      upcast_attention: false
      resnet_time_scale_shift: "default"
      resnet_skip_time_act: false
      resnet_out_scale_factor: 1.0
      
      # Kernel Configuration
      conv_in_kernel: 3
      conv_out_kernel: 3
      
      # Batch Configuration
      bs: 1
    instances:
      # Standard SDXL Configuration
      sdxl_base:
        sample_size: 128
        cross_attention_dim: 2048
        transformer_layers_per_block: [1, 2, 10, 10]
        attention_head_dim: [5, 10, 20, 20]
        block_out_channels: [320, 640, 1280, 1280]
        
      # Reduced Configuration for Testing
      sdxl_small:
        sample_size: 64 
        cross_attention_dim: 1024
        transformer_layers_per_block: [1, 1, 4, 4]
        attention_head_dim: [4, 8, 16, 16]
        block_out_channels: [160, 320, 640, 640]
        
      # Micro Configuration for Development  
      sdxl_micro:
        sample_size: 32
        cross_attention_dim: 512
        transformer_layers_per_block: [1, 1, 1, 1] 
        attention_head_dim: [4, 8, 16, 16]
        block_out_channels: [64, 128, 256, 256]

  # SDXL VAE AutoencoderKL
  - api: TTSIM
    name: vae_autoencoder
    basedir: workloads
    module: VAEAutoencoder.py
    params:
      # Core VAE Configuration
      in_channels: 3                    # Input RGB channels
      out_channels: 3                   # Output RGB channels  
      latent_channels: 4                # Latent space channels (before doubling for mean/logvar)
      sample_size: 512                  # Input image size (512x512 for SDXL)
      scaling_factor: 0.13025           # SDXL VAE scaling factor
      
      # Encoder Configuration
      down_block_types: ["DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D", "DownEncoderBlock2D"]
      block_out_channels: [128, 256, 512, 512]  # Progressive channel increase
      layers_per_block: 2               # ResNet layers per encoder block
      
      # Decoder Configuration  
      up_block_types: ["UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D", "UpDecoderBlock2D"]
      layers_per_decoder_block: 3       # ResNet layers per decoder block
      
      # Normalization and Activation
      norm_num_groups: 32               # GroupNorm groups
      norm_eps: 1e-6                    # GroupNorm epsilon
      act_fn: "silu"                    # SiLU activation
      
      # Architecture Settings
      double_z: true                    # Output 2x latent channels (mean + logvar)
      mid_block_add_attention: true     # Use attention in middle block
      num_attention_heads: 1            # Attention heads in middle block
      
      # Batch Configuration
      bs: 1                             # Batch size
      
    instances:
      # Full SDXL VAE Configuration
      vae_base:
        sample_size: 512
        latent_channels: 4
        block_out_channels: [128, 256, 512, 512]
        layers_per_block: 2
        layers_per_decoder_block: 3
        scaling_factor: 0.13025
        
      # Reduced VAE for Testing
      vae_small:
        sample_size: 256
        latent_channels: 4
        block_out_channels: [64, 128, 256, 256]
        layers_per_block: 2
        layers_per_decoder_block: 2
        scaling_factor: 0.13025
        
      # Micro VAE for Development
      vae_micro:
        sample_size: 128
        latent_channels: 4
        block_out_channels: [32, 64, 128, 128]
        layers_per_block: 1
        layers_per_decoder_block: 2
        scaling_factor: 0.13025
        
      # Tiny VAE for Quick Testing
      vae_tiny:
        sample_size: 64
        latent_channels: 4
        block_out_channels: [32, 64, 128, 128]  # Use 32 as minimum for better divisibility
        layers_per_block: 1
        layers_per_decoder_block: 1
        scaling_factor: 0.13025
        mid_block_add_attention: false  # Disable attention for simplicity
        norm_num_groups: 8              # 32/8=4, 64/8=8, 128/8=16 channels per group


  # Dual Text Encoder workloads (Ground Truth SDXL Architecture)
  - api: TTSIM
    name: dual_text_encoder
    basedir: workloads
    module: DualTextEncoder.py
    params:
      vocab_size: 1000
      max_seq_length: 32
      hidden_size_1: 768      # CLIPTextModel
      intermediate_size_1: 3072
      num_attention_heads_1: 12
      hidden_size_2: 1280     # CLIPTextModelWithProjection
      intermediate_size_2: 5120
      num_attention_heads_2: 20
      projection_dim: 1280
      num_hidden_layers: 6    # Reduced for TTSIM efficiency
      bs: 1
    instances:
      dual_tiny:
        vocab_size: 500
        max_seq_length: 16
        hidden_size_1: 384
        intermediate_size_1: 1536
        hidden_size_2: 640
        intermediate_size_2: 2560
        projection_dim: 640
        num_hidden_layers: 3
        bs: 1
      dual_small:
        vocab_size: 1000
        max_seq_length: 32
        hidden_size_1: 768
        intermediate_size_1: 3072
        hidden_size_2: 1280
        intermediate_size_2: 5120
        projection_dim: 1280
        num_hidden_layers: 6
        bs: 1
  # SDXL Complete Pipeline Integration
  - api: TTSIM
    name: sdxl_complete_pipeline
    group: sdxl_pipeline
    basedir: workloads
    module: SDXLPipeline.py
    params:
      # Global pipeline settings
      bs: 1
      num_inference_steps: 1
      
      # VAE configuration (Production VAE - ~83M parameters, TTSIM compatible)
      vae_in_channels: 3
      vae_latent_channels: 4
      vae_sample_size: 256
      vae_block_out_channels: [256, 512, 1024, 1024]  # Maximum channels, single layer
      vae_layers_per_block: 1  # TTSIM compatible (single layer per block)
      vae_scaling_factor: 0.13025
      
      # CLIP configuration (Dual Text Encoder - aligned with UNet cross-attention dim)
      clip_vocab_size: 49408
      clip_max_seq_length: 77
      # Text encoder 1 (CLIPTextModel)
      clip_hidden_size: 512              # Text encoder 1 output: 512-dim
      clip_intermediate_size: 2048
      clip_num_attention_heads: 8
      # Text encoder 2 (CLIPTextModelWithProjection)  
      clip_hidden_size_2: 1536           # Text encoder 2 output: 1536-dim (512+1536=2048 total)
      clip_intermediate_size_2: 6144
      clip_num_attention_heads_2: 24
      clip_projection_dim: 1536
      clip_num_hidden_layers: 6
      clip_max_position_embeddings: 77
      
      # UNet configuration (Ground Truth SDXL Standards)
      unet_sample_size: 128              # Ground truth base: 128 * 8 = 1024px output
      unet_in_channels: 4
      unet_out_channels: 4
      cross_attention_dim: 2048          # Ground truth: dual encoder output 768+1280=2048
      block_out_channels: [320, 640, 1280, 1280]    # Ground truth SDXL channels
      down_block_types: ["CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "DownBlock2D"]
      up_block_types: ["UpBlock2D", "CrossAttnUpBlock2D", "CrossAttnUpBlock2D", "CrossAttnUpBlock2D"]
      transformer_layers_per_block: [1, 2, 10, 10]  # Ground truth SDXL layers
      attention_head_dim: [5, 10, 20, 20]           # Ground truth SDXL attention heads
      layers_per_block: 2
      act_fn: "silu"
      norm_num_groups: 32
      addition_embed_type: "text_time"
      addition_time_embed_dim: 256
      projection_class_embeddings_input_dim: 2816   # 1280 pooled + 1536 time_ids
      
    instances:
      sdxl_micro:
        bs: 1
        vae_sample_size: 128
        unet_sample_size: 32               # 32 * 8 = 256px output (reasonable for micro)
        clip_max_seq_length: 32
        # Dual text encoder config for micro (scaled down)
        clip_hidden_size: 256               # Text encoder 1: 256-dim
        clip_intermediate_size: 1024
        clip_num_attention_heads: 4
        clip_hidden_size_2: 256             # Text encoder 2: 256-dim (256+256=512 total)
        clip_intermediate_size_2: 1024
        clip_num_attention_heads_2: 4
        clip_projection_dim: 256
        clip_num_hidden_layers: 3
        block_out_channels: [80, 160, 320, 320]       # Scaled down from [320,640,1280,1280]
        transformer_layers_per_block: [1, 1, 2, 2]    # Scaled down from [1,2,10,10]
        attention_head_dim: [4, 8, 16, 16]            # Reasonable scaled down heads
        cross_attention_dim: 512                      # 256+256=512 for micro dual encoder
        projection_class_embeddings_input_dim: 1792   # 256 pooled + 1536 time_ids
      sdxl_small:
        bs: 1
        vae_sample_size: 256
        unet_sample_size: 64               # 64 * 8 = 512px output (good for small)
        clip_max_seq_length: 77
        # Dual text encoder config for small (half scale)
        clip_hidden_size: 384               # Text encoder 1: 384-dim (half of 768)
        clip_intermediate_size: 1536
        clip_num_attention_heads: 6
        clip_hidden_size_2: 640             # Text encoder 2: 640-dim (half of 1280)
        clip_intermediate_size_2: 2560
        clip_num_attention_heads_2: 10
        clip_projection_dim: 640
        clip_num_hidden_layers: 6
        block_out_channels: [160, 320, 640, 640]      # Half scale from [320,640,1280,1280]
        transformer_layers_per_block: [1, 1, 5, 5]    # Half scale from [1,2,10,10]
        attention_head_dim: [4, 8, 16, 16]            # Reasonable heads for small
        cross_attention_dim: 1024                     # 384+640=1024 for small dual encoder
        projection_class_embeddings_input_dim: 2176   # 640 pooled + 1536 time_ids
      sdxl_base:
        bs: 1
        vae_sample_size: 512
        unet_sample_size: 128             # Ground truth: 128 * 8 = 1024px output
        clip_max_seq_length: 77
        # Dual text encoder config for base (Ground Truth SDXL)
        clip_hidden_size: 768               # Text encoder 1: 768-dim (CLIP ViT-L)
        clip_intermediate_size: 3072
        clip_num_attention_heads: 12
        clip_hidden_size_2: 1280            # Text encoder 2: 1280-dim (OpenCLIP ViT-bigG)
        clip_intermediate_size_2: 5120
        clip_num_attention_heads_2: 20
        clip_projection_dim: 1280
        clip_num_hidden_layers: 6           # Reduced for TTSIM efficiency  
        block_out_channels: [320, 640, 1280, 1280]
        transformer_layers_per_block: [1, 2, 10, 10]
        attention_head_dim: [5, 10, 20, 20]
        cross_attention_dim: 2048           # 768 + 1280 = 2048 (Ground Truth)
        projection_class_embeddings_input_dim: 2816  # 1280 pooled + 1536 time_ids (Ground Truth)
        num_inference_steps: 20

  # SDXL Individual Component Testing
  - api: TTSIM
    name: sdxl_component_vae
    basedir: workloads
    module: SDXLPipeline.py
    params:
      component_type: "vae"
      in_channels: 3
      latent_channels: 4
      sample_size: 128
      block_out_channels: [64, 128]
      scaling_factor: 0.13025
      bs: 1
    instances:
      vae_tiny:
        sample_size: 64
        block_out_channels: [32, 64]
        bs: 1
      vae_small:
        sample_size: 128
        block_out_channels: [64, 128]
        bs: 1

  - api: TTSIM
    name: sdxl_component_clip
    basedir: workloads
    module: SDXLPipeline.py
    params:
      component_type: "clip"
      vocab_size: 1000
      max_seq_length: 32
      hidden_size: 256
      intermediate_size: 1024
      num_attention_heads: 4
      max_position_embeddings: 32
      model_type: "standard"
      bs: 1
    instances:
      clip_tiny:
        vocab_size: 500
        max_seq_length: 16
        hidden_size: 128
        intermediate_size: 512
        num_attention_heads: 2
        bs: 1
      clip_small:
        vocab_size: 1000
        max_seq_length: 32
        hidden_size: 256
        intermediate_size: 1024
        num_attention_heads: 4
        bs: 1

  - api: TTSIM
    name: sdxl_component_unet
    basedir: workloads
    module: SDXLPipeline.py
    params:
      component_type: "unet"
      sample_size: 32
      in_channels: 4
      out_channels: 4
      cross_attention_dim: 768
      block_out_channels: [128, 256, 512, 512]
      down_block_types: ["CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "CrossAttnDownBlock2D", "DownBlock2D"]
      up_block_types: ["UpBlock2D", "CrossAttnUpBlock2D", "CrossAttnUpBlock2D", "CrossAttnUpBlock2D"]
      transformer_layers_per_block: [1, 1, 2, 2]
      attention_head_dim: [4, 8, 16, 16]
      layers_per_block: 2
      act_fn: "silu"
      norm_num_groups: 32
      bs: 1
    instances:
      unet_tiny:
        sample_size: 16
        block_out_channels: [64, 128, 256, 256]
        transformer_layers_per_block: [1, 1, 1, 1]
        attention_head_dim: [2, 4, 8, 8]
        bs: 1
      unet_small:
        sample_size: 32
        block_out_channels: [128, 256, 512, 512]
        transformer_layers_per_block: [1, 1, 2, 2]
        attention_head_dim: [4, 8, 16, 16]
        bs: 1

